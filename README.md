# Data Vending Machine

**Turn plain English into safe, paginated SQL results — with explanations.**

**Data Vending Machine** is an internal developer tool that lets you query a PostgreSQL database using plain English. It converts natural-language requests into SQL (via a local MCP tool and a fallback generator), executes queries in a paginated, memory-safe way, and returns both structured results and short natural-language explanations generated by a chat model.

> **Security reminder:** This tool executes SQL on your database. **Do not** expose it to untrusted users without strong access controls, least-privilege DB roles, and safe query policies.

---

## Table of contents

* [Why this exists](#why-this-exists)
* [Features](#features)
* [Architecture & design](#architecture--design)
* [Quick start](#quick-start)
* [Environment variables / .env example](#environment-variables--env-example)
* [Usage examples (NL -> SQL -> results)](#usage-examples-nl---sql---results)
* [How pagination & filtering work](#how-pagination--filtering-work)
* [UI details](#ui-details)
* [Security & operational notes](#security--operational-notes)
* [Troubleshooting & diagnostics](#troubleshooting--diagnostics)
* [Extending the app (suggested next steps)](#extending-the-app-suggested-next-steps)
* [Developer & contribution notes](#developer--contribution-notes)

---

# Why this exists

Teams often need ad-hoc access to data without building bespoke dashboards. Data Vending Machine bridges that gap by:

* letting non-SQL-savvy developers or analysts ask natural-language questions,
* converting those to SQL with an agent + fallback generator,
* executing queries safely (paginated, column-filtered), and
* returning both raw data and a concise natural-language explanation.

It’s intended for internal, trusted-user use — not as a public query endpoint.

---

# Features

* Natural-language → SQL conversion using an MCP tool and a fallback generator.
* Persistent agent & model to reduce latency between requests.
* Paginated query execution to avoid loading huge result sets into memory.
* Column chooser & server-side search filters (ILIKE) so you only fetch what you need.
* Tabs for **Raw JSON**, **Pretty Table** (paginated), and **Explanation** (model-generated).
* SQL syntax highlighting and editable SQL before execution.
* Safe executor runs using the same Python interpreter as Streamlit (`sys.executable`).
* Optional non-paginated execution for small queries / exports (use with caution).

---

# Architecture & design

High-level components:

1. **Streamlit app (`streamlit_app.py`)**
   Web UI. Handles prompt collection, shows agent reply, displays SQL, runs paginated queries, shows results and explanations, and manages UI state (`st.session_state`).

2. **MCP agent (local tool: `postgres_get_query.py`)**
   A tool exposed to the agent via `MultiServerMCPClient` that can translate natural-language to SQL. The app both uses the agent and falls back to calling the fallback `postgres_get_query.get_query()` helper.

3. **Executor (`postgres_execute_query.py`)**
   CLI helper that reads SQL on stdin and prints JSON output (rows). Streamlit runs this via a subprocess using `sys.executable`, so the same virtualenv/interpreter is used.

4. **Chat model (DeepSeek/OpenAI via `langchain_openai.ChatOpenAI`)**
   Used for generating natural-language explanations from the returned result set.

Design choices:

* **Paginated fetch**: the app wraps user SELECT queries in `WITH user_query AS (<user_sql>)` and uses `COUNT(*)` + `LIMIT/OFFSET` to fetch pages. This avoids reading tens of thousands of rows into memory at once.
* **Column chooser and server-side search**: columns are chosen from the current page’s columns and search is implemented via `ILIKE` conditions on specified columns (server-side).
* **Session persistence**: agent/model objects are stored in `st.session_state` to avoid re-creating them for each request (speeds up interaction).
* **Safety-first defaults**: pagination on by default; confirmation when running potentially destructive SQL is required (unless you intentionally use non-paginated mode).

---

# Quick start

1. Clone the repo:

```bash
git clone <your-repo-url> data-vending-machine
cd data-vending-machine
```

2. (Recommended) Create & activate a virtual environment:

```bash
python -m venv .venv
source .venv/bin/activate     # macOS / Linux
.venv\Scripts\activate        # Windows
```

3. Install dependencies:

```bash
pip install -r requirements.txt
```

4. Create a `.env` file (see example below), then run the app:

```bash
python -m streamlit run streamlit_app.py
```

5. Open `http://localhost:8501` in your browser.

---

# Environment variables / .env example

Create `.env` in repo root:

```
DEEPSEEK_API_KEY=sk-...
PGHOST=localhost
PGPORT=5432
PGUSER=myuser
PGPASSWORD=mypassword
PGDATABASE=mydb
```

* `DEEPSEEK_API_KEY` — API key used by the chat model adapter (DeepSeek/OpenAI token used via langchain bindings).
* Postgres vars — used by `postgres_execute_query.py` to open a DB connection.

---

# Usage examples (NL -> SQL -> results)

**Example 1 — Show tables**

* NL: `show me tables`
* SQL produced:

```sql
SELECT table_name
FROM information_schema.tables
WHERE table_schema = 'public';
```

* Page shows `test_set_features` (example).

**Example 2 — Simple analytics**

* NL: `how many orders did we get last month`
* Agent may produce:

```sql
SELECT COUNT(*) AS orders_last_month
FROM orders
WHERE order_date >= date_trunc('month', now()) - INTERVAL '1 month'
  AND order_date < date_trunc('month', now());
```

**Example 3 — Filtered / paginated**

* NL: `show me users with "gmail.com" in their email`
* SQL produced:

```sql
SELECT id, email, created_at FROM users WHERE email ILIKE '%gmail.com%' ORDER BY created_at DESC;
```

* The app will fetch pages and allow you to choose which columns to display, apply additional ILIKE-based search, and navigate pages.

---

# How pagination & filtering work

When the app detects the SQL is a `SELECT` and **paginated fetch** is enabled:

1. The user SQL is wrapped into a CTE:

```sql
WITH user_query AS (
  <USER_SQL_WITHOUT_TRAILING_SEMICOLON>
)
SELECT count(*) as cnt FROM user_query;
```

This gets the total row count (optional — may be slow for very complex queries).

2. To fetch a single page:

```sql
WITH user_query AS (
  <USER_SQL>
)
SELECT * FROM user_query
WHERE <search_filters_if_any>
LIMIT {page_size} OFFSET {page_idx * page_size};
```

3. Search filter (server-side) is built as ILIKE across selected columns, e.g.:

```sql
(first_name ILIKE '%john%' OR email ILIKE '%john%')
```

**Why this pattern?**

* Avoids loading the entire result set into Streamlit memory.
* Keeps DB work on the server and streaming of only the needed rows to the UI.
* Permits column chooser, as column names are inferred from the returned page.

**Caveats**

* `COUNT(*)` on complex queries can be expensive. You can toggle off the total-count step if it’s a performance problem.
* Search filters are generated via string concatenation (escaped by doubling quotes). For production, prefer prepared statements.

---

# UI details

* **Agent form**: type natural-language request and send to the MCP agent.
* **SQL candidate**: the agent produced SQL (and fallback generator if agent reply omitted SQL). This SQL is editable before execution.
* **Execute section**:

  * Column chooser — pick which columns to render.
  * Search term — ILIKE applied across chosen columns.
  * Pagination controls — Prev / Next / Go to page.
  * Advanced: run full (non-paginated) SQL (dangerous for very large results).
* **Tabs**:

  * **Raw JSON** — raw executor JSON output.
  * **Pretty Table** — paginated table with your selected columns.
  * **Explanation** — short natural-language summary generated by the model (says "table empty" when no rows).

---

# Security & operational notes

**Important — do not expose to untrusted users.** Specific recommendations:

* Run the app inside a trusted network or behind authentication (Reverse proxy with auth, VPN, or Streamlit’s experimental auth).
* Use a **read-only** DB role for the executor if possible — it should not have schema-altering privileges.
* Limit accessible schemas/tables by DB roles or views.
* Avoid constructing SQL from untrusted input when possible; prefer prepared statements and parameterized queries. The current app uses string building for the search filter for simplicity.
* Restrict model/agent usage with API key management and audit logs.
* If deploying publicly, require user authentication & strong RBAC.

---

# Troubleshooting & diagnostics

* **Button seems to do nothing**: run Streamlit from the same interpreter and check the terminal logs:

  ```bash
  python -m streamlit run streamlit_app.py
  ```

  Also ensure `postgres_execute_query.py` is in the current working directory.

* **Executor returns nothing**: run executor standalone:

  ```bash
  echo "SELECT table_name FROM information_schema.tables WHERE table_schema='public';" | python postgres_execute_query.py --run-sql
  ```

* **Agent/model creation takes long or errors**: ensure `DEEPSEEK_API_KEY` is set and network access to the model API is permitted.

* **Very slow count query**: toggle off the "get total rows" behavior in the UI (or add `LIMIT`/`OFFSET` without counting).

* **Streamlit memory / CPU spikes with large tables**: keep paginated fetch ON and reduce `Rows per page`.

If you hit a bug, paste the terminal logs and the Streamlit app debug block (`session_state` info) into an issue and include the SQL you asked to run.

---

# Extending the app (suggested next steps)

* **Server-side sorting** (`ORDER BY`): let the user choose a sort column and direction; include it in the paginated page query.
* **Type-aware filters**: detect column types and show date-range pickers or numeric range sliders.
* **CSV export / streaming**: stream large query results to CSV using `LIMIT/OFFSET` loops or server-side cursors to avoid memory exhaustion.
* **Audit log**: persist executed queries and users in a small audit DB for traceability.
* **Prepare statements & parameterization**: replace string-concatenated search filters with parameterized queries to mitigate injection risk.
* **Authentication & role-based access control**: protect the app with OAuth / SSO and map users to DB roles.
* **Background job queue** for heavy exports (Celery / RQ) — start background job and email / link when complete.

---

# Developer & contribution notes

* Use the existing `requirements.txt` to install dependencies.
* Code style: aim for small, testable helper functions (e.g., SQL-wrapping and search-clause builders).
* Add unit tests for SQL-building utilities (important: ensure escaping & name validation).
* When contributing, open an issue describing the change, then a PR. Add tests for new helpers.

Suggested development commands:

```bash
# run Streamlit for manual testing
python -m streamlit run streamlit_app.py

# run executor standalone (quick test)
echo "SELECT 1;" | python postgres_execute_query.py --run-sql
```

---
